% For easier proof-reading, use the single-column, double-spaced layout:
\documentclass{netsec2012}

% Final Paper use double-column, normal line spacing. Comment the
% above line and uncomment the following line when you are writing
% Full paper and Final paper!  
%\documentclass[cameraready]{netsec2012}

\begin{document}

%=========================================================

\title{T-61.3050 Data Challenge Submission}

\author{Olli Jarva \& Jarno Rantanen \\
        Aalto University School of Science \\
	\texttt{olli@jarva.fi} \texttt{\&} \texttt{jarno@jrw.fi}}
\maketitle

%==========================================================

\section{Solution in a nutshell}

We're using Support Vector Machines (SVM's) for classifying the input data (instances of 
handwritten characters) into 26 classes corresponding to the english alphabet. Images are 
preprocessed by aligning them to the lower left corner. The SVM uses a Radial Basis Function 
(RBF) kernel, with chained second-level SVM's specialized in entries commonly misclassified in 
the training data. The optimal kernel function parameters are sought for using standard 
grid-search and validated using k-fold cross-validation. With k=20, we achieved 89.6\% 
classification performance over the training set.

\section{Detailed description}

SVM's were chosen for their relative ease of use, while still being a robust and well-performing 
machine learning method for high-dimensional data.  Implementing an SVM from scratch is 
relatively error-prone, so we're using the open-source \texttt{mlpy} Python library for a 
ready-made SVM implementation based on \texttt{libsvm}.

% TODO: Discuss the preprocessing..?

The RBF kernel function was chosen simply because our literature review suggested them as a reasonable
default choice with SVM.  Also having only two parameters to the SVM helps in its training,
as the search space is only two-dimensional (as opposed to the 4 dimensions of the polynomial kernel,
for example).
% TODO: Check this claim; is the polynomial kernel easy to train in some other method than naive grid search

The training data uncovered sets of characters commonly misclassified by the primary classifier. 
While the characters themselves weren't easily classified, it was fairly easy to confidently 
identify the \emph{sets}.  For each such set, the classification was then delegated to a 
second-level SVM, specifically trained against that subset of the original problem.  With the 
latest training runs, we found this to increase our classification performance by almost 0.4\%, 
which we considered quite satisfactory. Together the SVM's form a \emph{classifier tree} of 
height 2. Adding a third level to the \emph{classifier tree} made the code more complex, and we couldn't
distinguish any advantages from potential measurement errors.

Optimal parameters for the SVM (and the kernel function) present a maximization problem in a
2-dimensional search space.  Since an exhaustive search in this space isn't possible,
a basic grid search was employed.  The performance of the classifier tree in each grid point
was evaluated using k-fold cross-validation.  Cross-validation was chosen as it provides
reasonable protection against overfitting against the training data.

%============================================================

\end{document}

