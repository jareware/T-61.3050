% For easier proof-reading, use the single-column, double-spaced layout:
\documentclass{netsec2012}

% Final Paper use double-column, normal line spacing. Comment the
% above line and uncomment the following line when you are writing
% Full paper and Final paper!  
%\documentclass[cameraready]{netsec2012}

\begin{document}

%=========================================================

\title{T-61.3050 Data Challenge Submission}

\author{Olli Jarva (69124C) \& Jarno Rantanen (64913L) \\
        Aalto University School of Science \\
	\texttt{olli@jarva.fi} \texttt{\&} \texttt{jarno@jrw.fi}}
\maketitle

%==========================================================

\section{Solution in a nutshell}

We're using Support Vector Machines (SVM's) for classifying the input data (instances of 
handwritten characters) into 26 classes corresponding to the english alphabet. Images are 
preprocessed by aligning them to the lower left corner. The SVM uses a Radial Basis Function 
(RBF) kernel. The optimal kernel function parameters are sought for using 
a grid-search and validated using k-fold cross-validation. With k=20, we achieved 90.8\% 
classification performance over the training set (n=42152, \cite{training_set}).

\section{Detailed description}

SVM's were chosen for their relative ease of use, while still being a robust and well-performing 
machine learning method for high-dimensional data (and number of samples >> features).  Implementing an SVM from scratch is 
relatively error-prone, so we're using the open-source \texttt{mlpy} \cite{mlpy} Python library for a 
ready-made SVM implementation based on \texttt{libsvm} \cite{libsvm}.

We minimized the variance by aligning images to lower left corner. This resulted approximately 0.3\%
improvement in classification performance.

The Radial Basis Function (RBF) kernel was chosen simply because our literature review suggested 
them as a reasonable default choice with SVM \cite{libsvm_guide,svm_chemistry}.  Also having only two parameters to the SVM helps in 
its training, as the search space is only two-dimensional (as opposed to the 4 dimensions of the 
polynomial kernel, for example). 

Optimal parameters for the SVM (and the kernel function) present a maximization problem in a 
2-dimensional search space: $\gamma$ for kernel and $C$ for SVM penalty parameter.  Since an exhaustive search 
in this space isn't possible, a basic grid search was employed.  The performance of the classifier 
tree in each grid point was evaluated using k-fold cross-validation.  Cross-validation was chosen as 
it provides reasonable protection against overfitting against the training data.

We also tried building a \emph{classifier tree} by having secondary classifiers for commonly 
misclassified pairs. However, after carefully selecting primary classifier parameters $\gamma$ and 
$C$, single classifier performed better than the classifier tree.

%============================================================

\bibliography{bibtex_challenge}

\end{document}

