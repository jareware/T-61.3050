% For easier proof-reading, use the single-column, double-spaced layout:
\documentclass{netsec2012}

% Final Paper use double-column, normal line spacing. Comment the
% above line and uncomment the following line when you are writing
% Full paper and Final paper!  
%\documentclass[cameraready]{netsec2012}

\begin{document}

%=========================================================

\title{Single-character OCR using Support Vector Machines}

\author{Olli Jarva \& Jarno Rantanen \\
        Aalto University School of Science \\
	\texttt{olli@jarva.fi} \texttt{\&} \texttt{jarno@jrw.fi}}
\maketitle

%==========================================================

\begin{abstract}

This paper describes a solution to an optical character recognition problem for bitmap characters
using Support Vector Machines with an RBF kernel, including a description of RBF parameter search
and bitmap normalization.  Classification performance of 90.8\% was achieved against a given
training set of 40000 correctly labelled samples \cite{training_set}.

\vspace{3mm}
\noindent KEYWORDS: SVM, Support Vector Machine, RBF, OCR, Character Recognition

\end{abstract}

%============================================================




% Remember that one of the objectives of the course is to teach you how to apply machine learning methods in a principled way, instead of just running some black box method on data and hoping that it will perform well. Therefore in the report it is not enough, e.g., just to explain the algorithm you implemented. Typically, the report should also include a discussion of why you selected a particular method (pros and cons), the principles of the method, validation of your approach (e.g., feature selection, model complexity selection, validation, estimation of generalization error etc.), your conclusions and other relevant issues.
% The factual content of the report must be correct and relevant. There should be sufficient amount of essential information (the length of the answer is not merit in itself). The report should give an impression that you understand what you are doing and therefore you are able to apply what you have learned in the course to machine learning problems.
% You must substantiate all of your claims. For example, you can't claim that your method will generalize well to new data if you haven't shown this, e.g., experimentally, by logical (mathematical) argumentation or by appropopriate citation. You must make a clear distinction between facts, substantiated claims and opinions (opinions being unsubstantiated claims).



\section{Data set description}

The data set against which our solution was developed consisted of a provided set of 42152
black-and-white bitmaps, depicting hand-written instances of characters from the English alphabet
(that is, the task was to classify the bitmaps into 26 distinct categories).  Each bitmap was given
as a 16-by-8 image, in the form of a binary vector of length 128 (meaning an array of 128 ones or
zeroes).  The vectors were delivered in a text file, with the correct label associated with each
vector.

As an optional extra task, we were provided with the opportunity of participating in a competition
amongst different solutions to the same classification problem.  The competition was organized by
first providing only a subset of the training data (10000 vectors), using that to train a
classifier, and then calculating an error rate against the rest of the training data (which was not
yet made available at that time).  Our participation in the competition is discussed further in
Section \ref{ref:datachallenge}.

The same data set was used for both training our classifiers, and testing them afterwards.  The data
was split using k-fold cross-validation to minimize overfitting, while making the most of the
available data.  This testing technique is discussed in further detail in Section
\ref{ref:crossvalidation}.

%Training dataset:
%
%\begin{itemize}
%\item n=42152
%\item 16 x 8 black and white bitmaps
%\item Lowercase characters, n=26 (a-z)
%\end{itemize}
%
%Testing dataset:
%
%\begin{itemize}
%\item n=10000
%\item Same format
%\end{itemize}

\section{Method selection}

\begin{itemize}
\item What other options were there
\item Why we chose SVM?
\item A nod to why we think we chose correctly
\end{itemize}

\section{Support Vector Machines}

Support Vector Machines (SVM's) are a method for supervised machine learning, meaning they solve a
classification problem by creating a classifier function from a set of existing, labeled data
points.  This function can then be used to classify (or \emph{label}) subsequent data points
\emph{without} supervision.  In contrast to \emph{regression methods} which produce continuous
output, the output of a classifier function is always exactly one class into which the input data
point (likely) belongs.

In its most basic implementation, an SVM is trained with data labeled into exactly two separate
groups.  The resulting classifier is a \emph{binary one}, meaning it will classify subsequent input
into those same two categories.  If the input data points belong to a two-dimensional space, this
can be intuitively thought of separating the data points into two clusters.  To minimize the
potential for generalization error, the clusters should be separated by as wide a band as possible
(or in simpler terms, by a line, with as much space between the line and the nearest data points as
possible).
% TODO: cite http://www.ivanciuc.org/Files/Reprint/Ivanciuc_SVM_CCR_2007_23_291.pdf

SVM's generalize nicely into higher dimensional spaces.  In an n-dimensional input space, the two
classes are linearly separated by an (n-1)-dimensional hyperplane instead of a line.  They also
generalize into working with >2 classes by way of reducing the multi-class classification problem
into a set of binary classification problems.  This can be done, for example, by chaining the binary
classifiers so that the first classifies the input data as either belonging to class "A" or "other",
the second one to "B" or other, and so on.
% TODO: cite http://research.yahoo.com/files/multiclass_mcs_kaibo_05.pdf

The above works off the assumption that the sets being discriminated are linearly separable in the
input space.  With realistic data sets, however, this is often not the case.  To keep the sets
linearly separable (and thus the SVM approach applicable), the input space can be mapped into a much
higher-dimensional space.  The assumption is that with this added sparsity, a linearly separating
hyperplane can be found, even for problematic input data sets.
% TODO: cite Press, William H.; Teukolsky, Saul A.; Vetterling, William T.; Flannery, B. P. (2007). "Section 16.5. Support Vector Machines". Numerical Recipes: The Art of Scientific Computing (3rd ed.). New York: Cambridge University Press. ISBN 978-0-521-88068-8.

Such mappings can be achieved using what are known as \emph{kernel functions}.  Kernel functions
have special properties that, in addition to helping the linear separability, reduce the
computational load bearable.  Numerous kernel functions have been proposed in literature, and new
ones are being researched.  The performance characteristics of the functions are highly dependent on
the type of classification problem at hand, and the properties of the input space, and not all
kernels work for all problems.  There is, however, little theoretical base on how to \emph{choose} a
suitable kernel for a given data set, and thus it is in fact common to simply rely on empirical
methods and compare the performance some common kernels, choosing the one that best fits the
specific data set.
% TODO: cite http://www.ivanciuc.org/Files/Reprint/Ivanciuc_SVM_CCR_2007_23_291.pdf
% TODO: cite Press, William H.; Teukolsky, Saul A.; Vetterling, William T.; Flannery, B. P. (2007). "Section 16.5. Support Vector Machines". Numerical Recipes: The Art of Scientific Computing (3rd ed.). New York: Cambridge University Press. ISBN 978-0-521-88068-8.

%\begin{itemize}
%\item http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf
%\item http://www.ivanciuc.org/Files/Reprint/Ivanciuc\_SVM\_CCR\_2007\_23\_291.pdf
%\item RBF kernel: $K(x_i, x_j) = exp(-\gamma || x_i - x_j ||^2), \gamma > 0$
%\item "Unique Features of SVM's and Kernel Methods" in \cite{berwick09idiots}
%\end{itemize}
%
%Optimization problem $(x_i, y_i), i = 1, ..., l$ where $x_i$ is ... (\cite{libsvm_guide}):
%
%minimize $w,b,\xi$: $\frac{1}{2}w^Tw + C \sum_{i=1}^l\xi_i$
%
%subject to $y_i(w^T \phi(x_i) + b) \ge 1 - \xi_i, \xi_i \ge 0$

\section{Character preprocessing}

\begin{itemize}
\item Minimize noise by moving characters to bottom left corner. 0.5\% improvement
\end{itemize}

\section{RBF kernel parameter search}

$\gamma$ and $C$

\label{ref:crossvalidation}

\begin{itemize}
\item Initial search space 2**x for x in range(-15, 15)
\item Select best area for next round
\item Validate by taking final arguments and calculating error rates for +- few percent for both variables.
\end{itemize}

\section{Results and performance}

\label{ref:datachallenge}

\begin{itemize}
\item k-fold cross validation: k=20, error rate 11\%
\item k-fold cross validation: k=5, error rate 11.5\%
\item One iteration with training set n=40000 and validation set n=2152 about 17 min with 2.1GHz Xeon (single thread)
\item about 300MB of memory for training set n=42152
\item Predicting one character: about 2 milliseconds

\end{itemize}

\section{Quick comparison to other algorithms}

\begin{itemize}
\item kNN (+PCA/LDA)
\item ...?
\end{itemize}


\cite{albanese12mlpy}

%============================================================

\bibliography{references}
\end{document}

